{
  "hash": "1994cd2b7c7d90458b862428ad13df29",
  "result": {
    "markdown": "---\ntitle: \"Linear and Non Linear Regression\"\nauthor: \"Shadab Haque\"\ndate: \"2023-12-10\"\ncategories: [Regression]\nimage: \"image.jpg\"\n---\n\n# Title: Linear and Non Linear Regression.\n# Linear regression:\nLinear regression is an essential tool in predictive analysis, focusing on establishing a linear relationship between a target variable and one or more predictor variables. This relationship is articulated through a linear equation, enabling the generation of predictions from existing data.\n\nThe process begins with the collection of a dataset containing paired observations of both the dependent (target) and independent (predictor) variables. Once the data is gathered, it's crucial to thoroughly analyze it. This analysis typically involves visualizing the data to understand its distribution, checking for outliers, and identifying patterns.\n\nAfter understanding the data, the next step is to identify the dependent variable that needs to be predicted and the independent variables that will be used for this prediction. Following this, the dataset is split into two parts: a training set and a testing set. The training set is instrumental in developing the model, while the testing set is used to evaluate the model's performance.\n\nThe actual model construction in simple linear regression, where there is only one predictor variable, is represented by the equation y = mx + c. The objective here is to find the best-fit coefficients, typically using statistical methods or optimization algorithms such as the least squares method, which aim to minimize the difference between the predicted and actual values in the training set.\n\nAfter the model is built, it's tested using the test dataset. The effectiveness of the model is measured using evaluation metrics such as mean squared error (MSE), root mean squared error (RMSE), and the coefficient of determination (R-squared). The interpretation of the model's coefficients is critical as it helps in understanding the nature of the relationship between the variables. A positive coefficient indicates a direct relationship, while a negative coefficient suggests an inverse relationship.\n\nOnce the model's performance is deemed satisfactory, it can be used to make predictions on new, unseen data. However, it's essential to ensure that the model adheres to the basic assumptions of linear regression. These include a linear relationship between the variables, independence of errors, homoscedasticity (consistent variance of errors), and normally distributed error terms. By confirming these assumptions, the reliability of the model's predictions is bolstered.\n\n\n\nThe Diabetes Dataset:\n\nThis dataset is composed of several medical statistics related to diabetes from a group of patients. It encompasses a variety of features such as age, sex, body mass index, average blood pressure, and six blood serum measurements. The main goal in utilizing the Diabetes Dataset is usually to predict a quantitative measure of disease progression one year after the baseline.\n\nRidge and Lasso Regression: \nRidge and Lasso regression are two types of regularization techniques used in linear regression models to prevent overfitting, improve model prediction accuracy, and handle multicollinearity among predictor variables. Here's why they are important:\n\nPreventing Overfitting: In a linear regression model, especially with a large number of predictors, there is a risk of overfitting, where the model becomes too complex and starts capturing noise in the data rather than the actual underlying pattern. Overfitting leads to poor generalization on new, unseen data. Both Ridge and Lasso add a penalty term to the standard linear regression cost function, which helps in controlling model complexity and thus reduces the risk of overfitting.\n\nHandling Multicollinearity: Multicollinearity occurs when predictor variables in a regression model are highly correlated. This can lead to unstable estimates of regression coefficients, making the model sensitive to small changes in the model or data. Ridge regression, in particular, is well-suited for dealing with multicollinearity by shrinking the coefficients of correlated predictors.\n\nFeature Selection: Lasso regression (Least Absolute Shrinkage and Selection Operator) not only helps in reducing overfitting but also performs feature selection. Unlike Ridge, which never sets the coefficients exactly to zero, Lasso has the property of completely eliminating some of the coefficients by setting them to zero, effectively choosing a simpler model that involves only a subset of the predictors. This is particularly useful in scenarios with many features, allowing for a more interpretable model.\n\nShrinkage of Coefficients: Both Ridge and Lasso shrink the coefficients towards zero, but they do it in slightly different ways. Ridge regression uses L2 regularization (adding a penalty equal to the square of the magnitude of coefficients), which tends to shrink coefficients evenly. Lasso uses L1 regularization (adding a penalty equal to the absolute value of the magnitude of coefficients), which can shrink some coefficients more than others.\n\nImproving Model Prediction Accuracy: By adding a regularization term, both Ridge and Lasso can often lead to better prediction accuracy on new, unseen data, compared to standard linear regression, especially when the dataset has features that are not relevant or when it has features with high collinearity.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load the Diabetes dataset\ndiabetes_data = load_diabetes()\ndiabetes_df = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\ndiabetes_df['Progression'] = diabetes_data.target\n\n# Select features and target\nfeatures = diabetes_df.drop('Progression', axis=1)\ntarget = diabetes_df['Progression']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n# Ridge Regression\nridge_model = Ridge(alpha=0.5)\nridge_model.fit(X_train, y_train)\nridge_y_pred = ridge_model.predict(X_test)\nridge_mse = mean_squared_error(y_test, ridge_y_pred)\nridge_r2 = r2_score(y_test, ridge_y_pred)\n\n# Lasso Regression\nlasso_model = Lasso(alpha=0.05)\nlasso_model.fit(X_train, y_train)\nlasso_y_pred = lasso_model.predict(X_test)\nlasso_mse = mean_squared_error(y_test, lasso_y_pred)\nlasso_r2 = r2_score(y_test, lasso_y_pred)\n```\n:::\n\n\n# Visualization:\nVisualizing predictions through scatter plots is a crucial step to understand the effectiveness of our models. These plots, showcasing actual versus predicted values, help us discern the underlying patterns and discrepancies in the model's forecasts.\n\nNext, we'll delve into comparing the performance metrics of each model. This comparison is key to evaluating their ability to generalize and make accurate predictions on unseen data from the Diabetes dataset.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Visualization of Predictions\nplt.figure(figsize=(12, 6))\n\n# Ridge Regression Predictions\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, ridge_y_pred, alpha=0.7)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Ridge Regression Predictions')\n\n# Lasso Regression Predictions\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, lasso_y_pred, alpha=0.7)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Lasso Regression Predictions')\n\nplt.tight_layout()\nplt.show()\n\n# Print results\nprint(\"Ridge Regression Results:\")\nprint(f'Mean Squared Error: {ridge_mse}')\nprint(f'R-squared: {ridge_r2}')\n\nprint(\"\\nLasso Regression Results:\")\nprint(f'Mean Squared Error: {lasso_mse}')\nprint(f'R-squared: {lasso_r2}')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1142 height=566}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRidge Regression Results:\nMean Squared Error: 2917.176593478921\nR-squared: 0.4493973121295206\n\nLasso Regression Results:\nMean Squared Error: 2821.2485185844794\nR-squared: 0.467503262930439\n```\n:::\n:::\n\n\nDifference between Ridge and Lasso regression:\n\nPenalty Type: Ridge uses squared values of coefficients (L2) for the penalty, while Lasso uses absolute values (L1).\n\nFeature Selection: Lasso can perform feature selection by setting some coefficients to zero. Ridge, on the other hand, only reduces the size of the coefficients.\n\nModel Complexity: Lasso can yield simpler models (if some coefficients become zero), whereas Ridge tends to include all features in the final model but with reduced coefficients.\n\nBehavior with Highly Correlated Features: In the presence of highly correlated features, Ridge regression tends to distribute the coefficients among them, while Lasso arbitrarily selects one and shrinks the others to zero.\n\n# Non Linear Regression:\n\nNonlinear regression is a type of regression analysis where the relationship between the dependent variable and independent variables is modeled using a nonlinear function. This contrasts with linear regression, which assumes a straightforward linear relationship. Nonlinear regression can capture more complex interactions between variables, typically expressed as y=f(x,β)+ε.\n\nWhen applying nonlinear regression to the Diabetes dataset, we focus on understanding the intricate relationships between various medical factors and diabetes progression. This dataset, rich in medical statistics such as age, sex, BMI, blood pressure, and blood serum measurements, provides an ideal context for such complex analyses. The aim is to predict the disease progression metric using these features.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Import required libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load the Diabetes dataset\ndiabetes_data = load_diabetes()\ndiabetes_df = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\ndiabetes_df['Progression'] = diabetes_data.target\n\n# Select a feature (e.g., BMI) and the target variable\nfeatures = diabetes_df[['bmi']]\ntarget = diabetes_df['Progression']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n# Polynomial regression\nX_poly_train = X_train.copy()\nX_poly_test = X_test.copy()\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly_train = poly_features.fit_transform(X_poly_train)\nX_poly_test = poly_features.transform(X_poly_test)\n\n# Feature scaling\nscaler = StandardScaler()\nX_poly_train_scaled = scaler.fit_transform(X_poly_train)\nX_poly_test_scaled = scaler.transform(X_poly_test)\n\n# Ridge Regression with alpha = 0.5\nridge_model = Ridge(alpha=0.5)\nridge_model.fit(X_poly_train_scaled, y_train)\nridge_y_pred = ridge_model.predict(X_poly_test_scaled)\nridge_mse = mean_squared_error(y_test, ridge_y_pred)\nridge_r2 = r2_score(y_test, ridge_y_pred)\n\n# Lasso Regression with alpha = 0.05\nlasso_model = Lasso(alpha=0.05)\nlasso_model.fit(X_poly_train_scaled, y_train)\nlasso_y_pred = lasso_model.predict(X_poly_test_scaled)\nlasso_mse = mean_squared_error(y_test, lasso_y_pred)\nlasso_r2 = r2_score(y_test, lasso_y_pred)\n```\n:::\n\n\nVisualizing predictions is essential for evaluating the performance of our models. To do this, let's create a plot that compares the actual data with the predictions generated by both Ridge and Lasso models\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_test, y_test, color='black', label='Actual Data')\nplt.scatter(X_test, ridge_y_pred, color='red', label='Ridge Predictions')\nplt.title('Ridge Regression on Polynomial Features')\nplt.xlabel('BMI')\nplt.ylabel('Disease Progression')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test, y_test, color='black', label='Actual Data')\nplt.scatter(X_test, lasso_y_pred, color='blue', label='Lasso Predictions')\nplt.title('Lasso Regression on Polynomial Features')\nplt.xlabel('BMI')\nplt.ylabel('Disease Progression')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Print results\nprint(\"Ridge Regression Results:\")\nprint(f'Mean Squared Error: {ridge_mse}')\nprint(f'R-squared: {ridge_r2}')\n\nprint(\"\\nLasso Regression Results:\")\nprint(f'Mean Squared Error: {lasso_mse}')\nprint(f'R-squared: {lasso_r2}')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=1142 height=566}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRidge Regression Results:\nMean Squared Error: 4083.9460507273984\nR-squared: 0.22917533423408665\n\nLasso Regression Results:\nMean Squared Error: 4083.4776690735416\nR-squared: 0.22926373896012875\n```\n:::\n:::\n\n\nApplying polynomial regression with Ridge and Lasso regularization to the Diabetes dataset has revealed the complex relationships inherent in the data. Exploring the realm of non-linear regression, we uncover the subtle equilibrium between the adaptability of the model and the necessity of regularization.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}